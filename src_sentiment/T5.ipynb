{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RiPS_ws3_t5_test",
      "provenance": [],
      "collapsed_sections": [
        "HVxGfmEMCKs_",
        "RKNr7fgzcKpZ",
        "vfhlYUUV2NIh",
        "b3C13iabZvwK",
        "qdEgCwL7cIyi",
        "W4cfw8bMcNdA",
        "brPOSAkjNP5t",
        "Dhqigmiw2hVh",
        "0B4IhzEgO21B",
        "cANrUEXhO8QY",
        "DEWi6c-pGZV9",
        "GwdWdHG0RP5J",
        "iq8M8nbTSJlE",
        "vZ-YLmJyg64T",
        "hOxk-ZoJmamm",
        "aVfmE4O3Ku7H",
        "AgNV3TMzqSvj"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0rR76v2l2Cs"
      },
      "source": [
        "# NLP analysis of RiPS transcripts using T5 (run on Google Colab)\n",
        "# requirements: python3 -m pip install transformers[sentencepiece], pytorch_lightning\n",
        "# read in the csv file and print sentiment scores per sentence\n",
        "# https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment\n",
        "# https://github.com/google-research/text-to-text-transfer-transformer/issues/109"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJX4vkjj6wYz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V5cInhu42Wk"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDVQ04fGRb1v"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ufB99Mczz9"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoP58X9LdBYE"
      },
      "source": [
        "transd = '/content/drive/My Drive/Colab/RiPSws3transcript/'\n",
        "sentd = '/content/drive/My Drive/Colab/t5sent/'\n",
        "trans = ['comment']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQVvguIYdN-U"
      },
      "source": [
        "transf_list = ['2020-11-16fetch_AWS','2020-11-17fetch_AWS_v1','2020-11-18fetch_AWS_v1','2020-11-23fetch_AWS_v1',\n",
        "               'pepper_1','pepper_2','pepper_3','pepper_4','WS3_Fetch','WS3_Pepper','WS3_all',\n",
        "               '2020-11-16fetch_AWS_simu','2020-11-17fetch_AWS_v1_simu','2020-11-18fetch_AWS_v1_simu',\n",
        "               '2020-11-23fetch_AWS_v1_simu','pepper_1_simu','pepper_2_simu','pepper_3_simu','pepper_4_simu',\n",
        "               'WS3_Fetch_simu','WS3_Pepper_simu','WS3_all_simu','2020-11-16fetch_AWS_robo','2020-11-17fetch_AWS_v1_robo',\n",
        "               '2020-11-18fetch_AWS_v1_robo','2020-11-23fetch_AWS_v1_robo','pepper_1_robo','pepper_2_robo','pepper_3_robo',\n",
        "               'pepper_4_robo','WS3_Fetch_robo','WS3_Pepper_robo','WS3_all_robo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up2sugQ0dRZV"
      },
      "source": [
        "# load model pre-trained with the IMDB database\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyYywS_QeiU6"
      },
      "source": [
        "# sentiment analysis with T5\n",
        "def get_sentiment(text):\n",
        "\tinput_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "\toutput = model.generate(input_ids=input_ids,max_length=2)\n",
        "\tdec = [tokenizer.decode(ids) for ids in output]\n",
        "\tlabel = dec[0]\n",
        "\treturn label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec-C9b4GelRK"
      },
      "source": [
        "# process all transcripts\n",
        "for transf in transf_list:\n",
        "\t# read in transcripts\n",
        "\tdf = pd.read_csv((transd+transf+'.csv'),usecols=trans)\n",
        "\t\n",
        "\t# apply T5 pretrained sentiment analysis tools\n",
        "\tsentscores = []\n",
        "\tfor sentence in df.comment:\n",
        "\t\tscore = get_sentiment(sentence)\n",
        "\t\tsentscores.append(score)\n",
        "\n",
        "\t# print to file\n",
        "\twith open((sentd+transf+'_sentscore.csv'),'w') as outf:\n",
        "\t\twr = csv.writer(outf,delimiter='\\n')\n",
        "\t\twr.writerow(sentscores)\n",
        "\n",
        "\tprint('Completed:',transf)\n",
        "\n",
        "print('All files processed!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}